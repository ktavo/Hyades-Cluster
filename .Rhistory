install.packages(plotrix)
install.packages("plotrix")
install.packages("plotrix")
install.packages("plotrix")
library("plotrix", lib.loc="C:/Program Files/R/R-3.4.0/library")
install.packages("plotrix")
setInternet2(set = TRUE)
setInternet2(use = TRUE)
install.packages("plotrix")
Modelos
Modelos<-2010:2016
Modelos
Ventas<-c(2,4,0,9,3,7,6)
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(9))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(7))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(1))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(9))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(1))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(10))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(100))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(7))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(9))
hist(PESO, col=maroon1, density=18, border= bluevliolet)
hist(PESO,col="maroon1",breaks=seq(0,85,5),density=18,angle=70)
no.gar=c(258,280)
gar.si=c(184,719)
mat=rbind(no.gar,gar.si)
colnames(mat)=c("no.cons","si.cons")
mosaicplot(mat,col=terrain.colors(2:3),main="Gráfico de mosaicos")
mosaicplot(mat,col=terrain.colors(2:3),main="Gráfico de mosaicos")
install.packages(plotrix)
install.packages("plotrix")
read.csv2("E:/UBA/Análisis Inteligente de Datos/1- 8 de abril - 14 de abril/movies.csv")
moviesDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/1- 8 de abril - 14 de abril/movies.csv")
attach(moviesDB)
frec.tipo <- (table(3))
etiquetas<-c("Comedy"; "Romance"; "Children"; "Adventure")
etiquetas<-c("Comedy", "Romance", "Children", "Adventure")
pie3D(frec.tipo,labels=etiquetas,explode=0.3,labelcex=0.8,radius=1.5)
install.packages("pie3D")
ap <- available.packages()
ap
install.packages("pie3d")
install.packages("pie3D")
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.xls")
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.xls")
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.csv")
recepcionistasDB
recepcionistasDB
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.csv")
recepcionistasDB
View(recepcionistasDB)
total <-rowMeans(recepcionistasDB)
total <-rowMeans(recepcionistasDB, -ncol(1))
total <-rowMeans(recepcionistasDB[, -ncol(1)])
> recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2, 7)), na.rm = TRUE)
recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2, 7)), na.rm = TRUE)
recepSubset = subset(recepcionistasDB, select = c(2,7))
recepSubset
View(recepSubset)
View(recepSubset)
View(recepSubset)
remove(recepSubset)
View(recepSubset)
View(recepcionistasDB)
recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2; 7)), na.rm = TRUE)
recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2: 7)), na.rm = TRUE)
View(recepcionistasDB)
recepcionistasDB$juez1 <- rowMeans(subset(recepcionistasDB, select = c(2: 4)), na.rm = TRUE)
recepcionistasDB$juez2 <- rowMeans(subset(recepcionistasDB, select = c(5: 7)), na.rm = TRUE)
View(recepcionistasDB)
recepcionistasDBNormalizada <- subset(recepcionistasDB, select = c(2:7))
recepcionistasDBNormalizada
view(recepcionistasDBNormalizada)
recepcionistasDBNormalizada <- scale(recepcionistasDBNormalizada)
view(recepcionistasDBNormalizada)
view(recepcionistasDB)
View(recepcionistasDBNormalizada)
meantest <- rowMeans(subset(recepcionistasDBNormalizada, select = c(1: 6)), na.rm = TRUE)
meantest
meantest <- rowMeans(recepcionistasDBNormalizada)
View(meantest)
View(recepcionistasDBNormalizada)
remove(meantest)
recepcionistasDBNormalizada <- scale(recepcionistasDBNormalizada, 0 ,1)
recepcionistasDBNormalizada <- rnorm(recepcionistasDB)
View(recepcionistasDBNormalizada)
View(recepcionistasDB)
recepcionistasDBNormalizada <- rnorm(subset(recepcionistasDB, select = c(2:7)))
View(recepcionistasDBNormalizada)
remove(recepcionistasDBNormalizada)
pointsRecepcionistas <- subset(recepcionistasDB, select = c(2:7))
View(pointsRecepcionistas)
normalizedPoints <- rnorm(pointsRecepcionistas)
View(normalizedPoints)
remove(normalizedPoints)
View(recepcionistasDB)
normalizedDB <- apply(recepcionistasDB, [,2], 1 , scale)
normalizedDB <- apply(recepcionistasDB[,2], 1 , scale)
install.packages("scales")
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas)
library("scales")
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas)
View(pointsRecepcionistas)
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas, to c (-1,1))
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas, to=c(-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, sum)
normalizedRecepcionistasDB
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale)
normalizedRecepcionistasDB
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale, (-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale(-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale, to=c(-1,1))
normalizedRecepcionistasDB
rowMeans <- apply(normalizedRecepcionistasDB, 2 , colMeans)
View(normalizedRecepcionistasDB)
pointsRecepcionistas
install.packages("ca")
library("ca")
fum = matrix  (c(4,2,3,2,4,3,7,4,25,10,12,4,18,24,33,13,10,6,7,2))
fum
View fum
View(fum)
fum = matrix  (c(4,2,3,2,4,3,7,4,25,10,12,4,18,24,33,13,10,6,7,2), nrow = 5, ncol = 4, byrow = TRUE)
View(fum)
tabum = addmargins(fum)
View(tabum)
colnames(fum) = c("NoFuma", "Poco", "Medio", "Mucho", "TotalFila")
colnames(tabum) = c("NoFuma", "Poco", "Medio", "Mucho", "TotalFila")
rownames(tabum) = c("G.Senior", "G.Junior", "EmpSenior", "EmpJunior", "Secretarira", "Total_Col")
View(tabum)
objeto = ca(tabum, nd=2)
plot(objeto, main="Biplot Simétrico")
Arbequina=c(34.5, 20.1, 21.8 ,18.2 ,19.5 ,20.2,22.5 ,23.9 ,22.1 ,24.2)
Carolea=c (16.4, 14.8, 17.8, 12.3, 11.9, 15.5, 13.4,16 ,15.8 ,16.2)
shapiro.test(Arbequina) # testeamos la normalidad de los datos
shapiro.test(CArolea) # testeamos la normalidad de los datos
shapiro.test(Carolea) # testeamos la normalidad de los datos
wilcox.test(Arbequina,Carolea, alternative="two.sided") # aplicamos el test de Mann Whitney
Wilcoxon bilateral
wilcox.test(Arbequina,Carolea, alternative="two.sided") # aplicamos el test de Mann Whitney
#Los datos no satisfacen el supuesto de normalidad distribucional, luego no puede aplicarse un test t.
library(RVAideMemoire)
install.packages("RVAdeMemoire")
install.packages("RVAideMemoire")
library(RVAideMemoire)
#te.aov<-aov(vitam  marca) # cargamos el análisis de la varianza en el objeto te.aov
#summary(te.aov) # pedimos la síntesis de la prueba
#bartlett.test(vitam,marca)
install.packages("Rcmdr)")
install.packages("Rcmdr")
install.packages("reshape2")
install.packages("car")
install.packages("car")
library("Rcmdr")
install.packages("nortest")
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Hyades")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Hyades")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Hyades")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
hipparcos <- read.table(file="datasets/hipparcos.csv", header=TRUE, sep=",")
hipparcos <- read.csv(file="datasets/hipparcos.xlsx", header=TRUE, sep=",")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Hyades")
rm(list=ls())
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Hyades")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
hipparcos <- read.csv(file="datasets/hipparcos.csv", sep=",")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=",")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=";")
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Hyades")
hipparcos <- read.csv(file="datasets/hipparcos.csv", header=TRUE, sep=";")
View(hipparcos)
View(hipparcos)
library(ggplot2)
#Realiza el análisis de componentes principales
hip.pca.cov = prcomp(hipparcos, center = TRUE, scale. = FALSE)
#Realiza el análisis de componentes principales con las variables estandarizadas
hip.pca.cor = prcomp(hipparcos, center = TRUE, scale. = FALSE)
hipparcos<- hipparcos(as.numeric(levels(RA_J2000))[f])
hipparcos<- hipparcos.as.numeric(levels(RA_J2000))[f]
hipparcos<- as.numeric(levels(RA_J2000))[f]
as.numeric(levels(hipparcos))[hipparcos]
hipparcos$RA_J2000 <- as.numeric(hipparcos$RA_J2000)
hipparcos <- as.numeric(hipparcos)
as.numeric(levels(hipparcos))[hipparcos]
hipparcos <- as.numeric(levels(hipparcos))[hipparcos]
hipparcos <- as.numeric((hipparcos))[hipparcos]
hipparcos$ <- as.numeric((hipparcos))[hipparcos]
hipparcos$RA_J2000 <- as.numeric(hipparcos$RA_J2000)
hipparcos$DE_J2000 <- as.numeric(hipparcos$DE_J2000)
hipparcos$plx <- as.numeric(hipparcos$plx)
hipparcos$Plx <- as.numeric(hipparcos$Plx)
library( "rpart" )
library( "data.table" )
library( "caret" )
install.packages("caret")
library( "caret" )
install.packages("caret",
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
library( "caret" )
#Modelo con libreria  rpart con busqueda  Grid Search
#Estimo la ganancia con  Repeated Random Sub Sampling Validation   ( Monte Carlo Cross Validation )
#Por favor, no desesperarse por la ESPANTOSA granularidad de la Grid Search
#notar el uso de CPU y memoria RAM
#Si este programa se corta,  se lo debe volver a correr y automaticamente retoma desde donde llego la vez anterior
#Estadisticos y actuarios no entren en panico porque estamos entrenando y evaluando en el mismo mes, ya vamos a mejorar
#exp-1100
#source( "~/cloud/cloud1/codigoR/rpart/rpart_tune_gridsearch_01.r" )
#limpio la memoria
rm( list=ls() )
gc()
library( "rpart" )
library( "data.table" )
library( "caret" )
switch ( Sys.info()[['sysname']],
Windows = { directory.include  <-  "M:\\codigoR\\include\\"
directory.work     <-  "M:\\work\\"
directory.plan     <-  "M:\\plan\\"
directory.datasets <-  "M:\\datasets\\"
},
Darwin  = { directory.include  <-  "~/dm/codigoR/include/"
directory.work     <-  "~/dm/work/"
directory.plan     <-  "~/dm/plan/"
directory.datasets <-  "~/dm/datasets/"
},
Linux   = { directory.include  <-  "~/cloud/cloud1/codigoR/include/"
directory.work     <-  "~/cloud/cloud1/work/"
directory.plan     <-  "~/cloud/cloud1/plan/"
directory.datasets <-  "~/cloud/cloud1/datasets/"
}
)
setwd( directory.include )
source( "metrica.r" )
#Parametros entrada de nuestro dataset
karchivo_entrada      <-  "201802.txt"
kcampos_separador     <-  "\t"
kcampo_id             <-  "numero_de_cliente"
kclase_nomcampo       <-  "clase_ternaria"
kclase_valor_positivo <-  "BAJA+2"
kcampos_a_borrar      <-  c( kcampo_id )
#Parametros  Repeated Random Sub Sampling Validation
ktraining_prob        <-  0.70
ksemilla_azar         <-  c( 102191, 200177, 410551, 552581, 892237 )
karchivo_salida       <-  "hyperparameter_GLOBAL.txt"
#estos valores se graban en el archivo de salida
kexperimento          <-  1100
kclase                <-  "ternaria"
kprograma             <-  "rpart_tune_gridsearch_01.r"
kalgoritmo            <-  "rpart"
kbusqueda             <-  "gridsearch"
kestimacion           <-  "Montecarlo"
kobservaciones        <-  "5 semillas"
#------------------------------------------------------
#Genera el modelo usando una semilla
modelo_rpart_uno = function( psemilla, pmaxdepth, pminbucket, pminsplit, pcp )
{
set.seed( psemilla )
inTraining        <-  createDataPartition( dataset[ , get(kclase_nomcampo)],   p = ktraining_prob, list = FALSE)
dataset_training  <-  dataset[  inTraining, ]
dataset_testing   <-  dataset[ -inTraining, ]
# generacion del modelo
formula  <-  formula( paste(kclase_nomcampo, "~ .") )
t0       <-  Sys.time()
modelo   <-  rpart( formula,   data = dataset_training,  xval=0, maxdepth=pmaxdepth, minbucket=pminbucket, minsplit=pminsplit, cp=pcp )
t1       <-  Sys.time()
tiempo <-  as.numeric(  t1 - t0, units = "secs")
#aplico el modelo a datos nuevos
testing_prediccion  <- predict(  modelo, dataset_testing , type = "prob")
# calculo la ganancia normalizada  en testing
gan <-  fmetrica_ganancia_rpart( testing_prediccion[, kclase_valor_positivo ],  dataset_testing[ , get(kclase_nomcampo)] ) / ( 1- ktraining_prob )
# calculo el AUC en testing
auc <- fmetrica_auc_rpart( testing_prediccion[ ,kclase_valor_positivo],  dataset_testing[ , get(kclase_nomcampo)] )
return(  list( "ganancia"=gan,  "tiempo"= tiempo,  "auc"=auc )  )
}
#------------------------------------------------------
#corre  rpart  usando  las semillas, y promedia el resultado
modelo_rpart_ganancia = function( dataset, pmaxdepth, pminbucket, pminsplit, pcp  )
{
res  <-   lapply( ksemilla_azar, modelo_rpart_uno, pmaxdepth=pmaxdepth,  pminbucket=pminbucket, pminsplit=pminsplit, pcp=pcp )
return(  list( "ganancia" = unlist( lapply( res, '[[', "ganancia" ) ),
"tiempo"   = unlist( lapply( res, '[[', "tiempo" ) ),
"auc"      = unlist( lapply( res, '[[', "auc" ) )
)
)
}
#------------------------------------------------------
#cargo los datos
setwd( directory.datasets )
dataset <- fread( karchivo_entrada, header=TRUE, sep=kcampos_separador )
#borro las variables que no me interesan
dataset[ ,  (kcampos_a_borrar) := NULL    ]
#escribo los  titulos  del archivo salida
setwd( directory.work )
if( !file.exists( karchivo_salida) )
{
cat("experimento",
"metrica",
"metrica2",
"tiempo",
"parametros",
"fecha",
"clase", "programa", "algoritmo", "busqueda" , "estimacion",
"dataset_train", "dataset_test", "observaciones",
"\n", sep="\t", file=karchivo_salida, fill=FALSE, append=FALSE )
lineas_salida <- 0
} else
{
salida <-  read.table( karchivo_salida, header=TRUE, sep=kcampos_separador )
lineas_salida <- nrow( salida )
}
linea <- 1
for( vcp  in  c( 0, 0.0005,  0.001, 0.005 ) )
{
for( vminsplit  in  c(  2, 5, 10, 20, 50, 100, 200, 400, 500, 800, 1000 )  )
{
for( vminbucket  in  c( trunc(vminsplit/4), trunc(vminsplit/3) )  )
{
for(  vmaxdepth  in  c(  4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30 ) )
{
if( linea > lineas_salida )  #no volver a procesar si en la corrida anterior se llego a esa lina
{
res <- modelo_rpart_ganancia( dataset, pmaxdepth=vmaxdepth, pminbucket=vminbucket, pminsplit=vminsplit, pcp=vcp  )
#genero el string con los parametros
st_parametros = paste( "xval=",      0,          ", " ,
"maxdepth=",  vmaxdepth,  ", " ,
"minbucket=", vminbucket, ", " ,
"minsplit=",  vminsplit,  ", " ,
"cp=",        vcp,
sep = ""
)
cat( kexperimento,
mean(res$ganancia),
mean(res$auc),
sum(res$tiempo),
st_parametros,
format(Sys.time(), "%Y%m%d %H%M%S"),
kclase,
kprograma,
kalgoritmo,
kbusqueda,
kestimacion,
karchivo_entrada, karchivo_entrada,
kobservaciones,
"\n", sep="\t", file=karchivo_salida, fill=FALSE, append=TRUE
)
}
linea <- linea+1
}
}
}
}
#limpio la memoria
rm( list=ls() )
gc()
#salgo del R sin grabar el gigante entorno
quit( save="no" )
